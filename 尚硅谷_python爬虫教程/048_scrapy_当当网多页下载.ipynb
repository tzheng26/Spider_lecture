{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy_当当网多页下载\n",
    "\n",
    "接上节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/spiders/dang.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy_dangdang_045.items import ScrapyDangdang045Item\n",
    "\n",
    "class DangSpider(scrapy.Spider):\n",
    "    name = \"dang\"\n",
    "    # 如果是多页下载的话 那么必须要调整allowed_domains的范围 一般情况下只写域名\n",
    "    allowed_domains = [\"category.dangdang.com\"]\n",
    "    start_urls = [\"https://category.dangdang.com/cp01.01.02.00.00.00.html\"]\n",
    "\n",
    "    base_url = \"https://category.dangdang.com/pg\"\n",
    "    page = 1\n",
    "\n",
    "    def parse(self, response):\n",
    "        # pipelines.py    下载数据\n",
    "        # items.py``      定义数据结构\n",
    "\n",
    "        # src = //ul[@id=\"component_59\"]/li//img/@src\n",
    "        # name = //ul[@id=\"component_59\"]/li//img/@alt\n",
    "        # price = //ul[@id=\"component_59\"]/li//p[@class=\"price\"]/span[1]/text() # 第一个span\n",
    "        \n",
    "        # 所有的selector的对象都可以再次调用xpath方法\n",
    "        li_list = response.xpath('//ul[@id=\"component_59\"]/li')\n",
    "        for li in li_list:\n",
    "            # 图片有懒加载，这里要用data-original 而不是src（有的是src2）\n",
    "            src = li.xpath('.//img/@data-original').extract_first()\n",
    "            # 第一张图片和其他的图片标签属性不一样\n",
    "            # 第一张图片的src是可以使用的  其他的图片的地址是data-original\n",
    "            if src:\n",
    "                src = src\n",
    "            else:\n",
    "                src = li.xpath('.//img/@src').extract_first()\n",
    "\n",
    "            name = li.xpath('.//img/@alt').extract_first()\n",
    "            price = li.xpath('.//p[@class=\"price\"]/span[1]/text()').extract_first()\n",
    "            print(src, name, price)\n",
    "\n",
    "            book = ScrapyDangdang045Item(src=src, name=name, price=price)\n",
    "\n",
    "            # 获取一个book就将book交给pipelines\n",
    "            yield book\n",
    "    \n",
    "        \n",
    "# 每一页的爬取的业务逻辑都是一样的，所以我们只需要将执行的那个页的请求再次调用parse方法即可\n",
    "#   https://category.dangdang.com/pg2-cp01.01.02.00.00.00.html\n",
    "#   https://category.dangdang.com/pg3-cp01.01.02.00.00.00.html\n",
    "\n",
    "        if self.page < 100:\n",
    "            self.page += 1\n",
    "            url = self.base_url + str(self.page) + \"-cp01.01.02.00.00.00.html\"\n",
    "\n",
    "            # 怎么取调用parse方法\n",
    "            # scrapy.Request就是scrapy的get请求\n",
    "            # url 就是请求地址\n",
    "            # callback 是你要执行的那个函数，不允许加()\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
