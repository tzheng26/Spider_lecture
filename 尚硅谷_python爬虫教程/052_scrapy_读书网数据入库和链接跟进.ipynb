{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy_读书网数据入库和链接跟进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在linux系统终端中进入数据库，创建一个和刚才爬取的数据一样结构的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 先连到自己的mysql\n",
    "mysql -uroot -p\n",
    "\n",
    "# 创建数据库\n",
    "create database spider01 charset=utf8;\n",
    "\n",
    "# 使用数据库\n",
    "use spider01;\n",
    "\n",
    "# 创建表\n",
    "create table book(\n",
    "    id int primary key auto_increment,\n",
    "    name varchar(128),\n",
    "    src varchar(128);\n",
    ");\n",
    "\n",
    "# 查看表\n",
    "select * from book;\n",
    "# > Empty set (0.01 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 查看linux ip\n",
    "ifconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在settings.py中配置数据库信息\n",
    "\n",
    "# scrapy_readbook_051/scrapy_readbook_051/settings.py\n",
    "\n",
    "# 随便找个空的地方，添加如下配置\n",
    "\n",
    "# 参数有两个坑：一个端口号，一个字符集\n",
    "DB_HOST = '192.168.0.2' # mysql数据库服务器的ip\n",
    "DB_PORT = 3306 # mysql数据库服务器的端口 端口号是整数\n",
    "DB_USER = 'root' \n",
    "DB_PASSWORD = '123456'\n",
    "DB_NAME = 'spider01' # 数据库名\n",
    "# 注意：utf-8的杠-不允许写\n",
    "DB_CHARSET = 'utf8' # 数据库编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_readbook_051/scrapy_readbook_051/pipelines.py\n",
    "# 在pipelines.py中添加自己的管道\n",
    "\n",
    "class MysqlPipeline:\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.fp.write(str(item))\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在settings.py中配置管道\n",
    "# scrapy_readbook_051/scrapy_readbook_051/settings.py\n",
    "\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   \"scrapy_readbook_051.pipelines.ScrapyReadbook051Pipeline\": 300,\n",
    "   # MysqlPipeline\n",
    "   \"scrapy_readbook_051.pipelines.MysqlPipeline\": 301,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义编写MysqlPipeline\n",
    "# scrapy_readbook_051/scrapy_readbook_051/pipelines.py\n",
    "\n",
    "\n",
    "# 加载settings.py文件\n",
    "from scrapy.utils.project import get_project_settings\n",
    "# 链接sql需要的包\n",
    "import pymysql\n",
    "\n",
    "class MysqlPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        settings = get_project_settings()\n",
    "        # DB_HOST = '192.168.0.2' # mysql数据库服务器的ip\n",
    "        # # 端口号是整数\n",
    "        # DB_PORT = 3306 # mysql数据库服务器的端口\n",
    "        # DB_USER = 'root' \n",
    "        # DB_PASSWORD = '123456'\n",
    "        # DB_NAME = 'spider01' # 数据库名\n",
    "        # DB_CHARSET = 'utf8' # 数据库编码\n",
    "        self.host = settings['DB_HOST']\n",
    "        self.port = settings['DB_PORT']\n",
    "        self.user = settings['DB_USER']\n",
    "        self.password = settings['DB_PASSWORD']\n",
    "        self.name = settings['DB_NAME']\n",
    "        self.charset = settings['DB_CHARSET']\n",
    "        \n",
    "        self.connect()\n",
    "\n",
    "    # 需要本地安装pymysql-->pip install pymysql\n",
    "    def connect(self):\n",
    "        self.conn = pymysql.connect(\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            db=self.name,\n",
    "            charset=self.charset\n",
    "        )\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        sql = 'insert into book(name, src) values(\"{}\", \"{}\")'.format(item['name'], item['src'])\n",
    "        # 执行sql语句\n",
    "        self.cursor.execute(sql)\n",
    "        # 提交\n",
    "        self.conn.commit()\n",
    "\n",
    "        return item\n",
    "    \n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.cursor.close()\n",
    "        self.conn.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 现在可以运行，然后上数据库服务器上查数据\n",
    "\n",
    "mysql> select * from book;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：如果之前`scrapy_readbook_051/scrapy_readbook_051/spiders/read.py`中\n",
    "\n",
    "```python\n",
    "class ReadSpider(CrawlSpider):\n",
    "    name = \"read\"\n",
    "    allowed_domains = [\"www.dushu.com\"]\n",
    "    start_urls = [\"https://www.dushu.com/book/1188.html\"]\n",
    "\n",
    "    # 在本例中follow=False只爬到了前13页的数据\n",
    "    # follow=True可以爬取所有的数据\n",
    "    rules = (Rule(LinkExtractor(allow=r\"/book/1188_\\d+\\.html\"), callback=\"parse_item\", follow=True),)\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        img_list = response.xpath('//div[@class=\"bookslist\"]//img')\n",
    "\n",
    "        for img in img_list:\n",
    "            name = img.xpath('./@alt').extract_first()\n",
    "            src = img.xpath('./@data-original').extract_first()\n",
    "            \n",
    "            book = ScrapyReadbook051Item(name=name, src=src)\n",
    "            yield book\n",
    "\n",
    "```\n",
    "follow=False只爬到了前13页的数据\n",
    "\n",
    "follow=True可以爬取所有的数据\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
