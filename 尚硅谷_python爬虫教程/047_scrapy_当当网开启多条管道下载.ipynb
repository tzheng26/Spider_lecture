{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy_当当网开启多条管道下载\n",
    "\n",
    "接上一节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/pipelines.py\n",
    "\n",
    "# 使用pipeline管道下载数据\n",
    "# 首先要在settings.py中开启管道\n",
    "# ------------------------------\n",
    "# settings.py:\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "# ITEM_PIPELINES = {\n",
    "#     # 管道可以有很多个  管道是有优先级的    优先级的范围是1-1000 值越小优先级越高\n",
    "#    \"scrapy_dangdang_045.pipelines.ScrapyDangdang045Pipeline\": 300,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "# 如果想使用管道的话 那么就必须在settings中开启管道\n",
    "class ScrapyDangdang045Pipeline:\n",
    "    # 在爬虫文件开始之前就执行的方法\n",
    "    def open_spider(self, spider):\n",
    "        print('++++++++++++++++++++++++++')\n",
    "        self.fp = open('book.json', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "    # item就是yield后面的book对象\n",
    "    def process_item(self, item, spider):\n",
    "        # 以下这种模式不推荐    因为每传递过来一个对象 就会打开一次文件 对文件的操作过于频繁\n",
    "        # # (1) write 方法必须要写一个字符串，而不能是其他对象\n",
    "        # # (2) w模式 会每个对象都打开一次文件 覆盖之前的内容\n",
    "        # with open('book.json', 'a', encoding='utf-8') as fp:\n",
    "        #     fp.write(str(item))\n",
    "\n",
    "        # 推荐使用以下模式 在前面定义\n",
    "        self.fp.write(str(item))\n",
    "\n",
    "        return item\n",
    "    \n",
    "\n",
    "    # 在爬虫文件结束之后就执行的方法\n",
    "    def close_spider(self, spider):\n",
    "        self.fp.close()\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# 多条管道开启\n",
    "# (1) 定义管道类\n",
    "# (2) 在settings.py中开启管道 \n",
    "#    添加新的管道和优先级\n",
    "#    \"scrapy_dangdang_045.pipeline.DangDangDownloadPipeline\": 301,\n",
    "\n",
    "class DangDangDownloadPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # 下载图片\n",
    "        os.makedirs('books', exist_ok=True)\n",
    "        url = 'http:' + item.get('src')\n",
    "        filename = './books/' + item.get('name') + '.jpg'\n",
    "        urllib.request.urlretrieve(url=url, filename=filename)\n",
    "        return item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
