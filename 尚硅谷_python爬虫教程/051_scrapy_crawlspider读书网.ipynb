{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# scrapy_crawlspider读书网"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrawlSpider 案例\n",
    "\n",
    "需求：读书网数据入库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建项目：scrapy startproject 项目的名字  \n",
    "2. 跳转到spiders路径：cd ./项目的名字/项目的名字/spiders\n",
    "3. 创建爬虫类：scrapy genspider -t crawl 爬虫文件的名字 爬取的网站的域名\n",
    "4. items\n",
    "5. spiders\n",
    "6. settings\n",
    "7. pipelines\n",
    "\n",
    "    数据保存到本地  \n",
    "    数据保存到mysql数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy startproject scrapy_readbook_051\n",
    "\n",
    "cd scrapy_readbook_051/scrapy_readbook_051/spiders\n",
    "\n",
    "# 创建爬虫文件\n",
    "scrapy genspider -t crawl read https://www.dushu.com/book/1188.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_readbook_051/scrapy_readbook_051/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyReadbook051Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    src = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_readbook_051/scrapy_readbook_051/spiders/read.py\n",
    "\n",
    "\n",
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy_readbook_051.items import ScrapyReadbook051Item\n",
    "\n",
    "class ReadSpider(CrawlSpider):\n",
    "    name = \"read\"\n",
    "    allowed_domains = [\"www.dushu.com\"]\n",
    "\n",
    "    # 有坑，第一页经常不符合我们后面设置的匹配规则，导致最后爬取的少了第一页的数据\n",
    "    # start_urls = [\"https://www.dushu.com/book/1188.html\"]\n",
    "    start_urls = [\"https://www.dushu.com/book/1188_1.html\"]\n",
    "\n",
    "    # 在本例中follow=False只爬到了前13页的数据\n",
    "    # follow=True可以爬取所有的数据\n",
    "    rules = (Rule(LinkExtractor(allow=r\"/book/1188_\\d+\\.html\"), callback=\"parse_item\", follow=False),)\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        img_list = response.xpath('//div[@class=\"bookslist\"]//img')\n",
    "\n",
    "        for img in img_list:\n",
    "            name = img.xpath('./@alt').extract_first()\n",
    "            src = img.xpath('./@data-original').extract_first()\n",
    "            \n",
    "            book = ScrapyReadbook051Item(name=name, src=src)\n",
    "            yield book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到settings.py中打开管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_readbook_051/scrapy_readbook_051/pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class ScrapyReadbook051Pipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.fp = open('book.json', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.fp.write(str(item))\n",
    "        \n",
    "        return item\n",
    "    \n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.fp.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
