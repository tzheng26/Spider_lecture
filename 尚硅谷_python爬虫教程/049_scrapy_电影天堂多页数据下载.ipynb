{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy_电影天堂多页数据下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy startproject scrapy_movie_049\n",
    "\n",
    "cd scrapy_movie_049/scrapy_movie_049/spiders\n",
    "\n",
    "scrapy genspider mv https://www.dytt8.net/html/gndy/china/index.html\n",
    "\n",
    "scrapy genspider mv https://btwuji.com/html/gndy/china/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_movie_049/scrapy_movie_049/spiders/mv.py\n",
    "\n",
    "# 要第一页电影的名字和电影链接后一页中电影海报的图片\n",
    "\n",
    "import scrapy\n",
    "from scrapy_movie_049.items import ScrapyMovie049Item\n",
    "\n",
    "class MvSpider(scrapy.Spider):\n",
    "    name = \"mv\"\n",
    "    allowed_domains = [\"btwuji.com\"]\n",
    "    start_urls = [\"https://btwuji.com/html/gndy/china/index.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 要第一个的名字和第二页的图片\n",
    "\n",
    "        # 获取第一页各个电影的信息\n",
    "        a_list = response.xpath('//div[@class=\"co_content8\"]//td[2]//a[2]')\n",
    "        for a in a_list:\n",
    "            # 获取第一页的name 和 要点击的链接\n",
    "            name = a.xpath('./text()').extract_first()\n",
    "            src = a.xpath('./@href').extract_first()\n",
    "            # print(name, src)\n",
    "\n",
    "            # 第二页的地址（即电影的链接地址）\n",
    "            url = 'https://btwuji.com' + src\n",
    "            # print(url)\n",
    "\n",
    "            # 对第二页的链接发起访问 通过meta参数传递name 传递给parse_second\n",
    "            yield scrapy.Request(url, callback=self.parse_second, meta={'name': name})\n",
    "\n",
    "\n",
    "    def parse_second(self, response):\n",
    "        # 获取第二页的图片\n",
    "\n",
    "        # 有坑，这里span失败不了，返回的都是None\n",
    "        # 注意：如果拿不到数据的情况下，一定要检查你的xpath语法是否正确\n",
    "        # src = response.xpath('//div[@id=\"Zoom\"]/span/img/@src').extract_first()\n",
    "        src = response.xpath('//div[@id=\"Zoom\"]//img/@src').extract_first()\n",
    "        print(src)\n",
    "\n",
    "        # 接受到请求的那个meta参数的值\n",
    "        name = response.meta['name']\n",
    "\n",
    "        movie = ScrapyMovie049Item(src=src, name=name)\n",
    "\n",
    "        yield movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_movie_049/scrapy_movie_049/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyMovie049Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    src = scrapy.Field()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_movie_049/scrapy_movie_049/pipelines.py\n",
    "# 注意要在settings.py中开启这个管道\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class ScrapyMovie049Pipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.fp = open('movie.json', 'w', encoding='utf-8')\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.fp.write(str(item))\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
