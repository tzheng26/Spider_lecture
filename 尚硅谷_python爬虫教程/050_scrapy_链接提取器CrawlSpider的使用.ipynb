{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy_链接提取器CrawlSpider的使用\n",
    "\n",
    "Mysql\n",
    "\n",
    "(1) 下载 https://dev.mysql.com/downloads/\n",
    "\n",
    "(2) 安装 https://jingyan.baidu.com/album/d7130635f1c77d13fdf475df.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "pymysql的使用步骤\n",
    "\n",
    "1. pip install pymysql\n",
    "\n",
    "2. pymysql.connect(host, port, user, password, db, charset)\n",
    "\n",
    "3. conn.cursor() \n",
    "\n",
    "4. cursor.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrawlSpider\n",
    "\n",
    "1. 继承自scrapy.Spider\n",
    "\n",
    "2. 独门秘笈\n",
    "   \n",
    "    CrawlSpider可以定义规则，在解析html内容的时候，可以根据链接规则提取出指定的链接，然后再向这些链接发送请求。  \n",
    "    所以如果有需要跟进链接的需求，即爬取网页之后，需要提取链接再次爬取，使用CrawlSpider是非常合适的。\n",
    "\n",
    "3. 提取链接\n",
    "\n",
    "    链接提取器。在这里就可以写规则提取指定链接\n",
    "\n",
    "    ```python\n",
    "    scrapy.linkextractors.LinkExtractor(  \n",
    "        allow = (),             # 正则表达式            提取符合正则的链接  \n",
    "        deny = (),              # （不用）正则表达式    不提取符合正则的链接  \n",
    "        allow_domains = (),     # （不用）允许的域名  \n",
    "        deny_domains = (),      # （不用）不允许的域名  \n",
    "        restrict_xpaths = (),   # xpath，提取符合xpath规则的链接  \n",
    "        restrict_css = (),      # 提取符合选择器规则的链接  \n",
    "    )\n",
    "    ```\n",
    "\n",
    "4. 模拟使用\n",
    "   \n",
    "    正则用法：links1 = LinkExtractor(allow = r'list_23_\\d+\\.html')  \n",
    "    xpath用法：links2 = LinkExtractor(restrict_xpaths = r'//div[@class=\"xxx\"]')  \n",
    "    css用法：links3 = LinkExtractor(restrict_css = '.xxx')\n",
    "\n",
    "5. 提取链接\n",
    "\n",
    "    link.extract_links(response) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# 使用scrapy shell举例\n",
    "scrapy shell https://www.dushu.com/book/1188.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.linkextractors import LinkExtractor\n",
    "# 提取当前页面中符合我们的要求的链接\n",
    "\n",
    "link1 = LinkExtractor(allow=r'/book/1188_\\d+\\.html') # 正则表达式\n",
    "\n",
    "link1.extract_links(response)\n",
    "\n",
    "link2 = LinkExtractor(restrict_xpaths=r'//div[@class=\"pages\"]/a/@href') # xpath\n",
    "link2.extract_links(response)\n",
    "\n",
    "link3 = LinkExtractor(restrict_css='div.pages a') # css\n",
    "link3.extract_links(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
