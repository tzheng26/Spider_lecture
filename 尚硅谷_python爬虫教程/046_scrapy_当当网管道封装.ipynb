{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 046_scrapy_当当网管道封装\n",
    "\n",
    "接上一节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/spiders/dang.py\n",
    "import scrapy\n",
    "from scrapy_dangdang_045.items import ScrapyDangdang045Item\n",
    "\n",
    "class DangSpider(scrapy.Spider):\n",
    "    name = \"dang\"\n",
    "    allowed_domains = [\"category.dangdang.com\"]\n",
    "    start_urls = [\"https://category.dangdang.com/cp01.01.02.00.00.00.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # pipelines.py    下载数据\n",
    "        # items.py``      定义数据结构\n",
    "\n",
    "        # src = //ul[@id=\"component_59\"]/li//img/@src\n",
    "        # name = //ul[@id=\"component_59\"]/li//img/@alt\n",
    "        # price = //ul[@id=\"component_59\"]/li//p[@class=\"price\"]/span[1]/text() # 第一个span\n",
    "        \n",
    "        # 所有的selector的对象都可以再次调用xpath方法\n",
    "        li_list = response.xpath('//ul[@id=\"component_59\"]/li')\n",
    "        for li in li_list:\n",
    "            # 图片有懒加载，这里要用data-original 而不是src（有的是src2）\n",
    "            src = li.xpath('.//img/@data-original').extract_first()\n",
    "            # 第一张图片和其他的图片标签属性不一样\n",
    "            # 第一张图片的src是可以使用的  其他的图片的地址是data-original\n",
    "            if src:\n",
    "                src = src\n",
    "            else:\n",
    "                src = li.xpath('.//img/@src').extract_first()\n",
    "\n",
    "            name = li.xpath('.//img/@alt').extract_first()\n",
    "            price = li.xpath('.//p[@class=\"price\"]/span[1]/text()').extract_first()\n",
    "            print(src, name, price)\n",
    "\n",
    "            # items.py定义的数据结构\n",
    "            book = ScrapyDangdang045Item(src=src, name=name, price=price)\n",
    "\n",
    "            # 获取一个book就将book交给pipelines\n",
    "            yield book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyDangdang045Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    # 通俗的说就是你要下载的数据都有什么\n",
    "\n",
    "    # 图片\n",
    "    src = scrapy.Field()\n",
    "    # 书名\n",
    "    name = scrapy.Field()\n",
    "    # 价格\n",
    "    price = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/pipelines.py\n",
    "\n",
    "# 使用pipeline管道下载数据\n",
    "# 首先要在settings.py中开启管道\n",
    "# ------------------------------\n",
    "# settings.py:\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "# ITEM_PIPELINES = {\n",
    "#     # 管道可以有很多个  管道是有优先级的    优先级的范围是1-1000 值越小优先级越高\n",
    "#    \"scrapy_dangdang_045.pipelines.ScrapyDangdang045Pipeline\": 300,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "# 如果想使用管道的话 那么就必须在settings中开启管道\n",
    "class ScrapyDangdang045Pipeline:\n",
    "\n",
    "    # item就是yield后面的book对象\n",
    "    def process_item(self, item, spider):\n",
    "        # 以下这种模式不推荐    因为每传递过来一个对象 就会打开一次文件 对文件的操作过于频繁\n",
    "        # (1) write 方法必须要写一个字符串，而不能是其他对象\n",
    "        # (2) w模式 会每个对象都打开一次文件 覆盖之前的内容\n",
    "        with open('book.json', 'a', encoding='utf-8') as fp:\n",
    "            fp.write(str(item))\n",
    "        return item\n",
    "\n",
    "# 推荐使用下面的方法，避免频繁打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_dangdang_045/scrapy_dangdang_045/pipelines.py\n",
    "\n",
    "# 使用pipeline管道下载数据\n",
    "# 首先要在settings.py中开启管道\n",
    "# ------------------------------\n",
    "# settings.py:\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "# ITEM_PIPELINES = {\n",
    "#     # 管道可以有很多个  管道是有优先级的    优先级的范围是1-1000 值越小优先级越高\n",
    "#    \"scrapy_dangdang_045.pipelines.ScrapyDangdang045Pipeline\": 300,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "# 如果想使用管道的话 那么就必须在settings中开启管道\n",
    "class ScrapyDangdang045Pipeline:\n",
    "    # 在爬虫文件开始之前就执行的方法\n",
    "    def open_spider(self, spider):\n",
    "        print('++++++++++++++++++++++++++')\n",
    "        self.fp = open('book.json', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "    # item就是yield后面的book对象\n",
    "    def process_item(self, item, spider):\n",
    "        # 以下这种模式不推荐    因为每传递过来一个对象 就会打开一次文件 对文件的操作过于频繁\n",
    "        # # (1) write 方法必须要写一个字符串，而不能是其他对象\n",
    "        # # (2) w模式 会每个对象都打开一次文件 覆盖之前的内容\n",
    "        # with open('book.json', 'a', encoding='utf-8') as fp:\n",
    "        #     fp.write(str(item))\n",
    "\n",
    "        # 推荐使用以下模式 在前面定义\n",
    "        self.fp.write(str(item))\n",
    "\n",
    "        return item\n",
    "    \n",
    "\n",
    "    # 在爬虫文件结束之后就执行的方法\n",
    "    def close_spider(self, spider):\n",
    "        self.fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
